{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obiWLwqmiAqV",
        "outputId": "59f773a6-c4a3-416a-d2dd-a9f93008ccf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/MyDrive/CAPSTONE_DATASETS'"
      ],
      "metadata": {
        "id": "cnmVZLmwiABj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20718d5c-b692-40be-b5c7-c6a7e27c02f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALL_DOWN_SYNDROME_IMAGES  Deletion_syndrome  TEXTUAL_DATA\n",
            "Autism_Images\t\t  GMDB\t\t     TRAIN1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSpZkTXwd87V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f41ac3-e2ce-4c25-ad2a-c96f3d14f08f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total image paths found: 1265\n",
            "Total labels found: 1265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:03<00:00, 177MB/s]\n",
            "/tmp/ipython-input-3-893935155.py:91: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-3-893935155.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Mixed precision context\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 8.9801, Accuracy: 51.48%\n",
            "Validation Accuracy: 49.41%\n",
            "Model saved!\n",
            "Epoch [2/10], Loss: 0.8024, Accuracy: 51.38%\n",
            "Validation Accuracy: 49.41%\n",
            "Epoch [3/10], Loss: 0.7591, Accuracy: 50.79%\n",
            "Validation Accuracy: 49.80%\n",
            "Model saved!\n",
            "Epoch [4/10], Loss: 0.6953, Accuracy: 52.17%\n",
            "Validation Accuracy: 51.38%\n",
            "Model saved!\n",
            "Epoch [5/10], Loss: 0.7284, Accuracy: 50.30%\n",
            "Validation Accuracy: 50.99%\n",
            "Epoch [6/10], Loss: 0.6919, Accuracy: 54.94%\n",
            "Validation Accuracy: 56.52%\n",
            "Model saved!\n",
            "Epoch [7/10], Loss: 0.6838, Accuracy: 53.46%\n",
            "Validation Accuracy: 51.78%\n",
            "Epoch [8/10], Loss: 0.6711, Accuracy: 59.88%\n",
            "Validation Accuracy: 60.87%\n",
            "Model saved!\n",
            "Epoch [9/10], Loss: 0.6580, Accuracy: 58.60%\n",
            "Validation Accuracy: 66.01%\n",
            "Model saved!\n",
            "Epoch [10/10], Loss: 0.6518, Accuracy: 60.47%\n",
            "Validation Accuracy: 64.82%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.cuda.amp import autocast, GradScaler  # For mixed precision training\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define custom Dataset class to load grayscale images\n",
        "class GrayscaleDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"L\")  # Convert to grayscale\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Image transformations (resize, normalize, and minimal augmentation)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),  # Only horizontal flip for faster training\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale\n",
        "])\n",
        "\n",
        "# Load dataset paths and labels\n",
        "dataset_path_down = \"/content/drive/MyDrive/CAPSTONE_PROCESSED/TRAIN/AUTISTIC_GRAY_IMAGES\"\n",
        "dataset_path_healthy = \"/content/drive/MyDrive/CAPSTONE_PROCESSED/TRAIN/NON_AUTISTIC_GRAY_IMAGES\"\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Label 0: Autistic\n",
        "for filename in os.listdir(dataset_path_down):\n",
        "    if filename.endswith((\".jpg\", \".png\")):\n",
        "        image_paths.append(os.path.join(dataset_path_down, filename))\n",
        "        labels.append(0)\n",
        "\n",
        "# Label 1: Non-Autistic (Healthy)\n",
        "for filename in os.listdir(dataset_path_healthy):\n",
        "    if filename.endswith((\".jpg\", \".png\")):\n",
        "        image_paths.append(os.path.join(dataset_path_healthy, filename))\n",
        "        labels.append(1)\n",
        "\n",
        "print(f\"Total image paths found: {len(image_paths)}\")\n",
        "print(f\"Total labels found: {len(labels)}\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = GrayscaleDataset(train_paths, train_labels, transform=transform)\n",
        "val_dataset = GrayscaleDataset(val_paths, val_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)  # Increased batch size\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)  # Increased batch size\n",
        "\n",
        "# Load VGG19 pretrained model\n",
        "vgg19 = models.vgg19(pretrained=True)\n",
        "\n",
        "# Modify the first conv layer to accept grayscale (1 channel)\n",
        "vgg19.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "# Modify the final fully connected layer for 2-class output\n",
        "vgg19.classifier[6] = nn.Linear(vgg19.classifier[6].in_features, 2)\n",
        "\n",
        "# Move model to GPU if available\n",
        "vgg19.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg19.parameters(), lr=0.001)\n",
        "\n",
        "# Learning Rate Scheduler (if necessary, helps stabilize training and can speed up convergence)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Mixed Precision Scaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training function with Mixed Precision\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():  # Mixed precision context\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()  # Scale loss for mixed precision\n",
        "            scaler.step(optimizer)  # Step the optimizer\n",
        "            scaler.update()  # Update the scaler\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), \"best_vgg19_model_autism.pth\")\n",
        "            print(\"Model saved!\")\n",
        "\n",
        "# Train the model\n",
        "train_model(vgg19, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10)\n"
      ]
    }
  ]
}