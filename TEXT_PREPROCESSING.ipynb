{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d11625",
   "metadata": {},
   "source": [
    "PES2UG22CS542_SHRIHARSHA MOGRA_TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Make sure required NLTK resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# =========================\n",
    "# 1. Load Dataset\n",
    "# =========================\n",
    "file_path = \"/content/drive/MyDrive/textual_data/TEXTUAL_DATA/processed_disorder_symptoms.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# =========================\n",
    "# 2. Text Cleaning Function\n",
    "# =========================\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove numbers & special characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove stopwords & lemmatize\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df[\"symptom\"] = df[\"symptom\"].astype(str).apply(clean_text)\n",
    "\n",
    "# =========================\n",
    "# 3. Frequency Mapping\n",
    "# =========================\n",
    "frequency_mapping = {\n",
    "    \"Very frequent (99-80%)\": 4,\n",
    "    \"Frequent (79-30%)\": 3,\n",
    "    \"Occasional (29-5%)\": 2,\n",
    "    \"Very rare (<4-1%)\": 1\n",
    "}\n",
    "df[\"frequency\"] = df[\"frequency\"].map(frequency_mapping)\n",
    "\n",
    "# =========================\n",
    "# 4. Drop Missing Values\n",
    "# =========================\n",
    "df = df.dropna(subset=[\"symptom\", \"disorder\", \"frequency\"])\n",
    "\n",
    "# =========================\n",
    "# 5. Encode Target (Disorders)\n",
    "# =========================\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"disorder\"])\n",
    "\n",
    "# =========================\n",
    "# ✅ Final Preprocessed Data\n",
    "# =========================\n",
    "# Features:\n",
    "#   - df[\"symptom\"]    → cleaned symptom text\n",
    "#   - df[\"frequency\"]  → numeric frequency\n",
    "# Target:\n",
    "#   - df[\"label\"]      → encoded disorder labels\n",
    "#   - df[\"disorder\"]   → original disorder names\n",
    "\n",
    "print(\"✅ Preprocessing Complete\")\n",
    "print(f\"Samples: {len(df)}, Unique Disorders: {df['disorder'].nunique()}\")\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
