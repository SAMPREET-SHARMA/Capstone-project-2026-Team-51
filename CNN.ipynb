{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gehekCwi804u",
        "outputId": "5d30f0f4-10c7-446b-c1f8-7499123ae695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "collapsed": true,
        "id": "JYE9H_AZNQ1A",
        "outputId": "49cad726-8c6c-4864-8a49-3ea475bdad56"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.20.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHrcdyMINeci",
        "outputId": "ba1ff873-238f-42fd-aebf-08f9e96b875c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dlib in /usr/local/lib/python3.12/dist-packages (19.24.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install dlib\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cejsnchYNf74",
        "outputId": "43bf466a-88f7-45e5-9ad1-7bf5fd7c5cd6"
      },
      "outputs": [],
      "source": [
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2  # Unzip the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7orvvc7mOJF8"
      },
      "outputs": [],
      "source": [
        "# Image preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHL0qoQUOJJK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # 1. Image Loading and Resizing\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # 2. Normalization (Scaling to [0, 1])\n",
        "    image = image / 255.0\n",
        "\n",
        "    # 5. Image Denoising using Gaussian Blur\n",
        "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "    # 6. Histogram Equalization\n",
        "    gray_image = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
        "    equalized_image = cv2.equalizeHist(gray_image)\n",
        "    equalized_image = cv2.cvtColor(equalized_image, cv2.COLOR_GRAY2RGB) / 255.0\n",
        "\n",
        "    # 8. Data Type Conversion\n",
        "    image = equalized_image.astype(np.float32)\n",
        "\n",
        "    return image\n",
        "\n",
        "def process_images(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "            try:\n",
        "                processed_image = preprocess_image(input_path)\n",
        "                # Convert to 8-bit and save the image\n",
        "                cv2.imwrite(output_path, (processed_image * 255).astype(np.uint8))\n",
        "                print(f\"Processed and saved: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "# Paths to input and output folders\n",
        "input_folder = '/content/drive/MyDrive/CAPSTONE_DATASETS/Autism_Images/data/train/Non_Autistic'\n",
        "output_folder = '/content/drive/MyDrive/preprocessing prototype 2/train/Non_Autistic'\n",
        "\n",
        "# Process all images\n",
        "process_images(input_folder, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAIpGMdzP6Y9"
      },
      "outputs": [],
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2Hv02m2P6cD",
        "outputId": "e8ab4440-7746-4ab5-d2a2-d8227ef0afeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1327/1327 [01:46<00:00, 12.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmentation complete. Check: /content/drive/MyDrive/preprocessing prototype 2/train/Augmented/Autistic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Input: Grayscale images (output of previous script)\n",
        "input_path = '/content/drive/MyDrive/CAPSTONE_DATASETS/Autism_Images/data/train/Autistic'\n",
        "augmented_path = '/content/drive/MyDrive/preprocessing prototype 2/train/Augmented/Autistic'\n",
        "os.makedirs(augmented_path, exist_ok=True)\n",
        "\n",
        "def augment_and_save(image, filename, count):\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "\n",
        "    # Save original grayscale image\n",
        "    cv2.imwrite(os.path.join(augmented_path, f\"{base_name}_orig.jpg\"), image)\n",
        "\n",
        "    # Horizontal Flip\n",
        "    flipped = cv2.flip(image, 1)\n",
        "    cv2.imwrite(os.path.join(augmented_path, f\"{base_name}_flip.jpg\"), flipped)\n",
        "\n",
        "    # Rotation\n",
        "    for angle in [15, -15]:\n",
        "        (h, w) = image.shape\n",
        "        center = (w // 2, h // 2)\n",
        "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "        rotated = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
        "        cv2.imwrite(os.path.join(augmented_path, f\"{base_name}_rot{angle}.jpg\"), rotated)\n",
        "\n",
        "    # Zoom\n",
        "    for zoom_factor in [1.1, 1.2]:\n",
        "        zoomed = cv2.resize(image, None, fx=zoom_factor, fy=zoom_factor)\n",
        "        zh, zw = zoomed.shape\n",
        "        zh_crop = (zh - h) // 2\n",
        "        zw_crop = (zw - w) // 2\n",
        "        cropped_zoom = zoomed[zh_crop:zh_crop+h, zw_crop:zw_crop+w]\n",
        "        cv2.imwrite(os.path.join(augmented_path, f\"{base_name}_zoom{int(zoom_factor*10)}.jpg\"), cropped_zoom)\n",
        "\n",
        "    # Brightness adjustment\n",
        "    for factor in [0.8, 1.2]:  # darker and brighter\n",
        "        bright = np.clip(image * factor, 0, 255).astype(np.uint8)\n",
        "        cv2.imwrite(os.path.join(augmented_path, f\"{base_name}_bright{int(factor*10)}.jpg\"), bright)\n",
        "\n",
        "# Process all grayscale images\n",
        "for count, filename in enumerate(tqdm(os.listdir(input_path))):\n",
        "    if filename.endswith((\".jpg\", \".png\")):\n",
        "        img_path = os.path.join(input_path, filename)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        augment_and_save(img, filename, count)\n",
        "\n",
        "print(\"Augmentation complete. Check:\", augmented_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b-G7RezNo3n"
      },
      "outputs": [],
      "source": [
        "#cnn model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKaLrEWRNo_l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import cv2\n",
        "\n",
        "# Define dataset paths\n",
        "AUTISTIC_PATH = \"/content/drive/MyDrive/CAPSTONE PROCESSED/TRAIN/AUTISTIC\"\n",
        "HEALTHY_PATH = \"/content/drive/MyDrive/CAPSTONE PROCESSED/TRAIN/NON_AUTISTIC\"\n",
        "\n",
        "# Image parameters\n",
        "IMG_SIZE = 224  # Resize images to 224x224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Function to load images and labels\n",
        "def load_images_from_folder(folder, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith((\".jpg\", \".png\")):\n",
        "            img_path = os.path.join(folder, filename)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # Resize to 224x224\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load data\n",
        "autistic_images, autistic_labels = load_images_from_folder(AUTISTIC_PATH, 1)  # Label 1 for autistic\n",
        "healthy_images, healthy_labels = load_images_from_folder(HEALTHY_PATH, 0)  # Label 0 for healthy\n",
        "\n",
        "# Combine datasets\n",
        "X = np.concatenate((autistic_images, healthy_images), axis=0)\n",
        "y = np.concatenate((autistic_labels, healthy_labels), axis=0)\n",
        "\n",
        "# Normalize images (scale pixel values to 0-1)\n",
        "X = X / 255.0\n",
        "X = np.expand_dims(X, axis=-1)  # Add channel dimension (224, 224, 1)\n",
        "\n",
        "# Shuffle dataset\n",
        "from sklearn.utils import shuffle\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split dataset into train & validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# CNN Model\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Dropout to prevent overfitting\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), batch_size=BATCH_SIZE)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/CAPSTONE_MODEL/AUTISM_MODEL_MORE_EPOCS.h5\",include_optimizer=True)\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Bh9-K2OeOHM"
      },
      "outputs": [],
      "source": [
        "# Best model (with Leaky_Relu activation function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_MTFhV7eOSz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import cv2\n",
        "\n",
        "# Define dataset paths\n",
        "AUTISTIC_PATH = \"/content/drive/MyDrive/CAPSTONE PROCESSED/TRAIN/AUTISTIC\"\n",
        "HEALTHY_PATH = \"/content/drive/MyDrive/CAPSTONE PROCESSED/TRAIN/NON_AUTISTIC\"\n",
        "\n",
        "# Image parameters\n",
        "IMG_SIZE = 224  # Resize images to 224x224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Function to load images and labels\n",
        "def load_images_from_folder(folder, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith((\".jpg\", \".png\")):\n",
        "            img_path = os.path.join(folder, filename)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # Resize to 224x224\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load data\n",
        "autistic_images, autistic_labels = load_images_from_folder(AUTISTIC_PATH, 1)  # Label 1 for autistic\n",
        "healthy_images, healthy_labels = load_images_from_folder(HEALTHY_PATH, 0)  # Label 0 for healthy\n",
        "\n",
        "# Combine datasets\n",
        "X = np.concatenate((autistic_images, healthy_images), axis=0)\n",
        "y = np.concatenate((autistic_labels, healthy_labels), axis=0)\n",
        "\n",
        "# Normalize images (scale pixel values to 0-1)\n",
        "X = X / 255.0\n",
        "X = np.expand_dims(X, axis=-1)  # Add channel dimension (224, 224, 1)\n",
        "\n",
        "# Shuffle dataset\n",
        "from sklearn.utils import shuffle\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split dataset into train & validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# CNN Model with Leaky ReLU activation\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
        "    layers.LeakyReLU(alpha=0.01),  # Leaky ReLU instead of ReLU\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3)),\n",
        "    layers.LeakyReLU(alpha=0.01),  # Leaky ReLU instead of ReLU\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(128, (3,3)),\n",
        "    layers.LeakyReLU(alpha=0.01),  # Leaky ReLU instead of ReLU\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128),\n",
        "    layers.LeakyReLU(alpha=0.01),  # Leaky ReLU instead of ReLU\n",
        "    layers.Dropout(0.5),  # Dropout to prevent overfitting\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), batch_size=BATCH_SIZE)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/CAPSTONE_MODEL/AUTISM_MODEL_LEAKY_RELU_MORE_EPOCS.h5\", include_optimizer=True)\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGbZCIw-OQuZ"
      },
      "outputs": [],
      "source": [
        "# CNN with preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjeGTBg7OQxb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "\n",
        "# Define dataset paths\n",
        "AUTISTIC_PATH = \"/content/drive/MyDrive/preprocessing prototype/Autistic\"\n",
        "HEALTHY_PATH = \"/content/drive/MyDrive/preprocessing prototype/Non_autistic\"\n",
        "\n",
        "# Image parameters\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Function to load images and labels\n",
        "def load_images_from_folder(folder, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(('.jpg', '.png')):\n",
        "            img_path = os.path.join(folder, filename)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load data\n",
        "autistic_images, autistic_labels = load_images_from_folder(AUTISTIC_PATH, 1)\n",
        "healthy_images, healthy_labels = load_images_from_folder(HEALTHY_PATH, 0)\n",
        "\n",
        "# Combine datasets\n",
        "X = np.concatenate((autistic_images, healthy_images), axis=0)\n",
        "y = np.concatenate((autistic_labels, healthy_labels), axis=0)\n",
        "\n",
        "# Ensure images are in correct shape\n",
        "X = np.expand_dims(X, axis=-1)\n",
        "\n",
        "# Shuffle dataset\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split dataset into train & validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# CNN Model\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=BATCH_SIZE)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/CAPSTONE_MODEL/AUTISM_MODEL_PROCESSED.h5\", include_optimizer=True)\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbi2Gj0UPlXy"
      },
      "outputs": [],
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5Ru6WCD-Pla5",
        "outputId": "1d391879-684d-4ccc-9e0c-193e5b0f8c44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 512ms/step\n",
            "Image: 134.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Image: 068.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 099.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Image: 116.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 136.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 109.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Image: 128.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 123.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 066.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 065.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 069.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Image: 063.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 074.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Image: 070.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Image: 121.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Image: 115.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 106.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 082.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 086.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 107.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 113.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Image: 112.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 089.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 104.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 110.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 090.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 096.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Image: 072.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 080.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "Image: 092.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 129.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 064.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Image: 071.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 132.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 097.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 075.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Image: 117.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 124.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Image: 058.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 091.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 137.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 084.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 108.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 060.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 056.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 078.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 093.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 076.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 061.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Image: 085.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Image: 053.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 077.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 031.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 120.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 135.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 055.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 105.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 122.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 003.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 027.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 057.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 062.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 111.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 100.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 126.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 094.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Image: 131.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 009.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Image: 081.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 079.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 119.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 050.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 051.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Image: 023.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 118.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Image: 114.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Image: 087.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 133.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 038.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 130.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 139.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 088.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Image: 044.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "Image: 016.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "Image: 073.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "Image: 014.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Image: 001.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "Image: 049.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Image: 030.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Image: 102.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "Image: 012.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Image: 007.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Image: 095.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Image: 138.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Image: 059.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "Image: 045.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "Image: 054.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "Image: 125.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Image: 022.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Image: 032.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Image: 010.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Image: 015.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "Image: 098.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 035.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 029.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 011.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 026.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 008.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 067.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 024.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 033.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 046.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 028.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Image: 039.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Image: 004.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Image: 040.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 025.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 037.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 127.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 018.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Image: 013.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 002.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Image: 052.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Image: 021.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Image: 047.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Image: 042.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 043.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 140.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 048.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 041.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 101.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 017.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 036.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Image: 005.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 019.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Image: 006.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Image: 083.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Image: 020.jpg, Prediction: Healthy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Image: 034.jpg, Prediction: Down Syndrome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Image: 103.jpg, Prediction: Down Syndrome\n",
            "\n",
            "Total Down Syndrome Predictions: 33\n",
            "Total Healthy Predictions: 107\n",
            "Accuracy : 76.42857142857142%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the model\n",
        "model = keras.models.load_model('/content/drive/MyDrive/CAPSTONE_MODEL/AUTISM_MODEL_SWISH.h5')\n",
        "\n",
        "# Folder path containing images\n",
        "input_folder = '/content/drive/MyDrive/CAPSTONE_DATASETS/Autism_Images/data/test/Non_Autistic'\n",
        "\n",
        "# Image processing parameters\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Counters for predictions\n",
        "down_syndrome_count = 0\n",
        "healthy_count = 0\n",
        "\n",
        "# Process each .jpg image in the folder\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.lower().endswith('.jpg'):  # Only read .jpg files\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Error loading image: {filename}\")\n",
        "            continue\n",
        "\n",
        "        # Preprocess image\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "        img = img / 255.0\n",
        "        img = np.expand_dims(img, axis=(0, -1))  # Add batch and channel dimensions\n",
        "\n",
        "        # Predict\n",
        "        prediction = model.predict(img)\n",
        "        label = 'Down Syndrome' if prediction[0][0] > 0.5 else 'Healthy'\n",
        "        print(f\"Image: {filename}, Prediction: {label}\")\n",
        "\n",
        "        # Update count\n",
        "        if label == 'Down Syndrome':\n",
        "            down_syndrome_count += 1\n",
        "        else:\n",
        "            healthy_count += 1\n",
        "\n",
        "# Final result\n",
        "print(f\"\\nTotal Down Syndrome Predictions: {down_syndrome_count}\")\n",
        "print(f\"Total Healthy Predictions: {healthy_count}\")\n",
        "temp = healthy_count/(healthy_count + down_syndrome_count);\n",
        "print(f\"Accuracy : {temp*100}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fIHcbXlSAxe"
      },
      "outputs": [],
      "source": [
        "# Count number of photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIi_cJSJaE6t"
      },
      "outputs": [],
      "source": [
        "# List files"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
